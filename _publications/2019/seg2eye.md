---
ref: seg2eye
title: "Content-Consistent Generation of Realistic Eyes with Style"
authors: Marcel Buehler, Seonwook Park, Shalini De Mello, Xucong Zhang, Otmar Hilliges
date: 2019-11-02
venue: "International Conference on Computer Vision Workshops (ICCVW)"
image: /assets/projects/seg2eye/teaser.mp4
external_project_page: 
video: 
talk: 
paper: https://files.ait.ethz.ch/projects/seg2eye/buehler2019iccvw.pdf
poster: 
data: 
code: https://github.com/mcbuehler/Seg2Eye
conference_url: https://research.fb.com/programs/the-2019-openeds-workshop-eye-tracking-for-vr-and-ar/
equal_contributions: 
award: "OpenEDS Image Generation Challenge Winner"
bibtex: "@inproceedings{Buehler2019ICCVW,
  author    = {Marcel C. Buehler and Seonwook Park and Shalini De Mello and Xucong Zhang and Otmar Hilliges},
  title     = {Content-Consistent Generation of Realistic Eyes with Style},
  year      = {2019},
  booktitle = {International Conference on Computer Vision Workshops (ICCVW)},
  location  = {Seoul, Korea}
}
"
---

<img class="fullcol" src="/assets/projects/seg2eye/style_walk.png" alt="Teaser-Picture" />

<p align="justify">
    <span class="figurecap">
Walking  the  style  latent  space  in  our  proposed method, Seg2Eye.  We extract latent style codes from two people and show the decodings of their linear interpolation.
    </span>
</p>
<hr />
        


<h3>Abstract</h3>
<p align="justify">
Accurately labeled real-world training data can be scarce, and hence recent works adapt, modify or generate images
to boost target datasets. However, retaining relevant details from input data in the generated images is challenging
and failure could be critical to the performance on the final task. In this work, we synthesize person-specific eye
images that satisfy a given semantic segmentation mask (content), while following the style of a specified person
from only a few reference images. We introduce two approaches, (a) one used to win the
<a href="https://research.fb.com/programs/openeds-challenge" title="OpenEDS Challenge Page" target="_blank">OpenEDS Synthetic Eye
  Generation Challenge</a> at <a href="http://iccv2019.thecvf.com/" title="ICCV 2019" target="_blank">ICCVW 2019</a>, and (b) a principled approach to solving the problem involving simultaneous
injection of style and content information at multiple scales. Our implementation is available on <a
  href="https://github.com/mcbuehler/Seg2Eye" title="GitHub Repository" target="_blank">GitHub</a>.

</p>
<hr />
    


<!--
<div class="fullcol">
<h3>Accompanying Video</h3>
    <br />
    <div class="video" align="center">
	<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/cLUHKYfZN5s?rel=0&amp;showinfo=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
    <hr />
    <br/>
    <br/>
</div>
-->

<!--
<div class="fullcol">
 <h3>Downloads</h3>
    To be released.
    <ul class="linklist">
        <li class="a-pdf"><a target="_blank" title="PDF" href="https://files.ait.ethz.ch/projects/InteractiveDebugger/FluidEdt-Ou-CHI2015.pdf">PDF</a></li>
        <li class="a-vid"><a target="_blank" href="https://files.ait.ethz.chprojects/InteractiveDebugger/FluidEdt-Ou-CHI2015.mp4" title="Download Video">Video (26 MB)</a></li>
        <li class="a-bib"><a target="_blank" title="BibTex" href="https://files.ait.ethz.ch/projects/InteractiveDebugger/FluidEdt-Ou-CHI2015.bib">BibTeX</a></li>
    </ul>
    <hr />
    <br/>
    <br/>
</div>
-->


<!--
<div class="fullcol">
    <h3>additional results</h3>
    <br/>
    <img class="halfcol" src="/assets/projects/deformables/bar_small.png" alt="Teaser-Picture" />
    <img class="halfcol" src="/assets/projects/deformables/organ_stacked_small.png" alt="Teaser-Picture" />
    <div class="halfcol">
        <p align="justify">
            <span class="figurecap">
                Top row: schematic sensor routings obtained using our tool with automatic sensor refinement.
                Middle row: fabricated device.
                Bottom row: Ground truth (gray) vs. reconstruction (orange). Insets show error on a heat map scale, with maximum error (white) at 22 mm (darker is better).
            </span>
        </p>
    </div>
    <div class="halfcol">
        <p align="justify">
            <span class="figurecap">
                Two example deformations of the organ pipe model designed with our method. Ground truth (gray) vs. reconstruction (orange).
            </span>
        </p>
    </div>
</div>
-->

<!--
<div class="fullcol">
    <br/><br/>
    <img class="fullcol" src="/assets/projects/deformables/sheet_squared_small.png" alt="Teaser-Picture" />
    <p align="justify">
        <span class="figurecap">
            Snapshots of the design process. Top Row: the user placed, refined,
            and edited four sensors (left); Reconstruction error is expected to be very low (right). Bottom row: Interaction
            with fabricated device (left) and ground truth comparison (right).
        </span>
    </p>
    <hr />
    <br/>
    <br/>
</div>
-->

<!-- This section is optional -->
<!--
<div class="fullcol">
    <h3>external links</h3>
    <p align="justify">
        <ul class="linklist">
        <li class="a-ext"><a target="_blank" title="link1" href="your_link_here">Your link here</a></li>
    </ul>
    </p>
    <hr />
    <br/>
    <br/>
</div>
-->

<h3>Acknowledgments</h3>
<p align="justify">
This work was supported in part by the ERC Grant OPTINT (StG-2016-717054).
</p>
<hr />
    


<h3>Downloads</h3>
<ul class="linklist">
    <li class="a-pdf"><a class="a-text-ext" title="PDF" href="https://files.ait.ethz.ch/projects/seg2eye/buehler2019iccvw_supplementary.pdf">PDF (Supplementary)</a></li>
    <!--<li class="a-vid"><a title="Video" href="https://files.ait.ethz.ch/projects/pictorial-gaze/park2018eccv.mp4">Video</a></li>-->
</ul>